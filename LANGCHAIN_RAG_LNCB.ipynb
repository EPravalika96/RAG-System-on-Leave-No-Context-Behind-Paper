{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98b52942-5c2b-4225-83ca-74798e1559c9",
   "metadata": {},
   "source": [
    "# **Building a QnA system**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6454e725-6c48-42c6-8ed4-698e53e90978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[SystemMessage(content='You are a Helpful AI Bot. \\n    You take the question from user and answer if you have the specific information related to the question. '), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='Aswer the following question: {question}\\n    Answer: '))])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    You take the question from user and answer if you have the specific information related to the question. \"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Aswer the following question: {question}\n",
    "    Answer: \"\"\")\n",
    "])\n",
    "\n",
    "chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fc5572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open(\"/Users/eleshalapravalika/Downloads/GEMINI AI TUTOR/gemini_key.txt\")\n",
    "key = file1.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98f6a02b-abe3-4475-b7fd-98acb2a320a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=key, \n",
    "                                   model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59048e6f-4c8e-44af-89fc-7d270a073654",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fee6ca72-6e97-4dc9-854f-f00d9ef3b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chat_template | chat_model | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c7b9555-5ccf-43cc-aa41-2d205a84ca54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Leave No Context Behind: A Potential Analysis\n",
      "\n",
      "Unfortunately, with my knowledge cutoff in November 2023, I don't have specific information about a paper titled \"Leave No Context Behind.\" There are a few possibilities and approaches we can take to find out more:\n",
      "\n",
      "**1. Identifying the Paper:**\n",
      "\n",
      "* **Title Search:** Try searching online databases like Google Scholar, Semantic Scholar, or research repositories like arXiv using the exact title \"Leave No Context Behind.\" This might lead you to the paper directly or related works.\n",
      "* **Keyword Search:**  If the exact title search doesn't work, try searching for keywords related to the paper's potential topic. For example, if you suspect the paper is about natural language processing, you could search for \"contextual language models,\" \"contextual embeddings,\" or \"context-aware NLP.\"\n",
      "* **Author Search:** If you know the author(s) of the paper, try searching for their names and see if the paper appears in their list of publications.\n",
      "\n",
      "**2. Exploring Potential Topics:**\n",
      "\n",
      "The phrase \"Leave No Context Behind\" suggests the paper might deal with the importance of context in a specific field. Here are a few potential areas:\n",
      "\n",
      "* **Natural Language Processing (NLP):**  Context is crucial in NLP tasks like machine translation, sentiment analysis, and text summarization. The paper might explore methods for incorporating context into NLP models or discuss the limitations of context-free approaches.\n",
      "* **Machine Learning (ML):**  Context can also be important in ML, particularly in areas like reinforcement learning or time series analysis. The paper might investigate how to represent and utilize contextual information in ML models.\n",
      "* **Human-Computer Interaction (HCI):**  Understanding user context is vital for designing effective and user-friendly interfaces. The paper might explore methods for capturing and utilizing user context in HCI applications.\n",
      "\n",
      "**3. Seeking Further Information:**\n",
      "\n",
      "* **Academic Communities:** Consider reaching out to researchers or communities specializing in the potential areas mentioned above. They might be familiar with the paper or able to point you in the right direction.\n",
      "* **Online Forums and Discussion Boards:** Platforms like Reddit or specialized forums related to NLP, ML, or HCI could be helpful. You can post a query about the paper and see if anyone has information.\n",
      "\n",
      "**I hope these suggestions help you find the information you're looking for!** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_input = \"Can you tell me about the Leave no context behind paper\"\n",
    "\n",
    "print(chain.invoke({\"question\": user_input}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c978113a-4bee-49db-acb2-c86471ffcf3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a document\n",
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"/Users/eleshalapravalika/Downloads/LEAVENOCONTEXTBEHIND.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e75b7bf3-c09a-4878-a08a-dde35e4816c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "# Split the document into chunks\n",
    "\n",
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19bc3109-9423-4871-9e9c-238cc31f5bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Chunks Embedding\n",
    "# We are just loading OpenAIEmbeddings\n",
    "\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key, \n",
    "                                               model=\"models/embedding-001\")\n",
    "\n",
    "# vectors = embeddings.embed_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "329ce0e9-3cfd-4731-a6ea-131fe3fcdc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the chunks in vector store\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Embed each chunk and load it into the vector store\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
    "\n",
    "# Persist the database on drive\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3246503-750c-4424-b180-4364d0adcdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting a Connection with the ChromaDB\n",
    "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1d73d70-f080-452e-a04f-430284d16858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'langchain_core.vectorstores.VectorStoreRetriever'>\n"
     ]
    }
   ],
   "source": [
    "# Converting CHROMA db_connection to Retriever Object\n",
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "print(type(retriever))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af4ab79-2972-4720-b92a-9f84a5c7ef0f",
   "metadata": {},
   "source": [
    "Now letâ€™s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b021bd3-539c-41b9-b6da-3f7ba56e52bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_docs = retriever.invoke(user_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78b5f482-4810-4a13-a5fe-db5486d441fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a3fff3f1-6c0f-41a7-99ac-744a63184d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprint.\n",
      "\n",
      "Under review.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "efc59f48-fdb8-4c17-9e96-90192bf3f649",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    You take the question related to Context from user and answer if you have the specific information related to the question.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Answer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: \n",
    "    {question}\n",
    "    \n",
    "    Answer: \"\"\")\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1845eabe-2e78-4503-99db-4f0ef3a668c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0771118b-08b7-436a-b65d-876691a7c84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\\n\\nBased on the context you provided, here\\'s a summary of the paper \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\":\\n\\n**Main Idea:**\\n\\nThis work introduces **Infini-attention**, a novel attention mechanism designed to efficiently handle both long and short-range contextual dependencies within Transformer models. This enables the processing of infinitely long contexts, overcoming limitations of standard attention mechanisms.\\n\\n**Key Contributions:**\\n\\n1. **Infini-attention Mechanism:** This powerful attention mechanism combines:\\n    * **Long-term compressive memory:**  Stores and retrieves relevant information from extensive past contexts.\\n    * **Local causal attention:** Focuses on recent context for capturing local dependencies. \\n2. **Minimal Modification:**  Infini-attention integrates seamlessly with existing Transformer architectures, requiring minimal changes to the standard scaled dot-product attention.\\n3. **Plug-and-Play:**  It supports continual pre-training and long-context adaptation, allowing models to learn from ever-growing datasets and adapt to new information efficiently.\\n\\n**How it Works:**\\n\\n* Infini-attention incorporates a compressive memory into the vanilla attention mechanism.\\n* It utilizes both masked local attention and long-term linear attention within a single Transformer block.\\n* This enables the model to access and process information from both recent and distant past contexts effectively.\\n\\n**Benefits:**\\n\\n* **Efficient Long-Context Modeling:**  Handles infinitely long sequences of data, overcoming limitations of traditional models.\\n* **Improved Performance:**  Leads to better performance on tasks requiring long-range context understanding. \\n* **Continual Learning:**  Enables models to continuously learn and adapt to new information without forgetting past knowledge. \\n\\n**Overall, Infini-attention presents a significant advancement in Transformer models, allowing them to efficiently process and learn from infinitely long contexts, opening doors to more powerful and versatile natural language processing applications.** \\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"Can you tell me about the Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85fd0b1e-1047-465c-abe9-48e271435333",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n",
       "\n",
       "Based on the context you provided, here's a summary of the paper \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\":\n",
       "\n",
       "**Main Idea:**\n",
       "\n",
       "This work introduces **Infini-attention**, a novel attention mechanism designed to efficiently handle both long and short-range contextual dependencies within Transformer models. This enables the processing of infinitely long contexts, overcoming limitations of standard attention mechanisms.\n",
       "\n",
       "**Key Contributions:**\n",
       "\n",
       "1. **Infini-attention Mechanism:** This powerful attention mechanism combines:\n",
       "    * **Long-term compressive memory:**  Stores and retrieves relevant information from extensive past contexts.\n",
       "    * **Local causal attention:** Focuses on recent context for capturing local dependencies. \n",
       "2. **Minimal Modification:**  Infini-attention integrates seamlessly with existing Transformer architectures, requiring minimal changes to the standard scaled dot-product attention.\n",
       "3. **Plug-and-Play:**  It supports continual pre-training and long-context adaptation, allowing models to learn from ever-growing datasets and adapt to new information efficiently.\n",
       "\n",
       "**How it Works:**\n",
       "\n",
       "* Infini-attention incorporates a compressive memory into the vanilla attention mechanism.\n",
       "* It utilizes both masked local attention and long-term linear attention within a single Transformer block.\n",
       "* This enables the model to access and process information from both recent and distant past contexts effectively.\n",
       "\n",
       "**Benefits:**\n",
       "\n",
       "* **Efficient Long-Context Modeling:**  Handles infinitely long sequences of data, overcoming limitations of traditional models.\n",
       "* **Improved Performance:**  Leads to better performance on tasks requiring long-range context understanding. \n",
       "* **Continual Learning:**  Enables models to continuously learn and adapt to new information without forgetting past knowledge. \n",
       "\n",
       "**Overall, Infini-attention presents a significant advancement in Transformer models, allowing them to efficiently process and learn from infinitely long contexts, opening doors to more powerful and versatile natural language processing applications.** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown as md\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c40b3de-b6e3-4e26-b27b-329167d2e1fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Compressive Memory Explained:\n",
       "\n",
       "Based on the context you provided, **compressive memory** is a unique approach to storing and recalling information that draws inspiration from the plasticity of biological neurons. Unlike traditional memory systems (like arrays) that grow in size with the amount of data, compressive memory utilizes a fixed number of parameters. This allows it to maintain bounded storage and computation costs, making it efficient even when dealing with large amounts of data. \n",
       "\n",
       "**Here's how it works:**\n",
       "\n",
       "* **Parameterized functions as memory:** Instead of simply storing data directly, compressive memory uses parameterized functions (mathematical representations with adjustable parameters) to encode and represent the information. \n",
       "* **Adding new information:** When new information is introduced, the parameters of these functions are adjusted and updated. The objective is to modify the functions in such a way that the original information can be accurately recovered later.\n",
       "* **Benefits:** This approach offers several advantages:\n",
       "    * **Efficiency:** By using a fixed number of parameters, compressive memory avoids the ever-increasing memory demands of traditional methods.\n",
       "    * **Bounded costs:**  Storage and computational costs remain manageable, making it suitable for resource-constrained environments.\n",
       "    * **Adaptability:** The ability to adjust parameters allows the memory to continuously learn and adapt to new information.\n",
       "\n",
       "**Current Challenges:**\n",
       "\n",
       "While the concept of compressive memory holds great promise, the context mentions that current large language models (LLMs) haven't yet found a way to implement it effectively in a practical setting. The challenge lies in striking the right balance between simplicity and the quality of information storage and retrieval.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is Compressive Memory?\")\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b673f4d0-9d59-4e82-8fdd-4c246a74af78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## LLM Continual Pre-training Explained: Adapting to Long-Context Information\n",
       "\n",
       "LLM Continual Pre-training focuses on adapting existing Large Language Models (LLMs) to handle **long-context information** effectively. This is crucial because standard LLMs often struggle with processing and understanding lengthy sequences of text.\n",
       "\n",
       "Here's a breakdown of the key elements involved:\n",
       "\n",
       "**1. Extending Attention Mechanisms:**\n",
       "\n",
       "*   Traditional LLMs use \"dot-product attention\" which has limitations when dealing with long sequences. \n",
       "*   This method is replaced with mechanisms like **Infini-attention** that are better suited for long-context scenarios.\n",
       "\n",
       "**2. Continued Pre-training on Long Sequences:**\n",
       "\n",
       "*   Existing LLMs are further trained on datasets containing text sequences exceeding 4,000 tokens. \n",
       "*   Examples of such datasets include PG19, Arxiv-math corpus, and lengthy sections from the C4 text dataset.\n",
       "\n",
       "**3. Segmenting Long Sequences:**\n",
       "\n",
       "*   To manage the computational challenges of processing extensive text sequences, the input is divided into segments. \n",
       "*   In the given context, a segment length (N) of 2,000 tokens is used throughout the experiments.\n",
       "\n",
       "**4. Lightweight Adaptation:**\n",
       "\n",
       "*   The pre-training process is designed to be lightweight, meaning it efficiently adapts the existing LLM without requiring extensive resources or retraining from scratch.\n",
       "\n",
       "**Benefits of LLM Continual Pre-training:**\n",
       "\n",
       "*   **Improved Performance on Long-Context Tasks:** LLMs become capable of understanding and responding to prompts or questions that require processing lengthy information sequences. \n",
       "*   **Enhanced Comprehension and Reasoning:** By considering a broader context, LLMs can achieve deeper comprehension and provide more insightful responses.\n",
       "*   **Efficient Adaptation:** Existing LLMs can be adapted to long-context scenarios without the need for extensive retraining, saving time and resources.\n",
       "\n",
       "**Examples of Applications:**\n",
       "\n",
       "*   **Summarizing lengthy documents or research papers**\n",
       "*   **Answering complex questions that require considering extensive background information**\n",
       "*   **Generating coherent and contextually relevant text in creative writing or dialogue systems** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\" Explain  LLM Continual Pre-training in detail\")\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ec89fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Efficient Infinite Context Transformers: A Summary Based on the Context\n",
       "\n",
       "The provided text seems to be discussing \"Efficient Infinite Context Transformers,\" likely referring to a specific model or architecture within the realm of Transformer models in machine learning. While the full details are not available, we can glean some key points:\n",
       "\n",
       "**Key Features:**\n",
       "\n",
       "* **Unbounded Context Window:** This model appears to address the limitations of traditional Transformers with fixed-length context windows. It can handle and process input sequences of theoretically infinite length, which is crucial for tasks requiring long-range dependencies, such as long document summarization or analyzing extensive time-series data. \n",
       "* **Bounded Memory Footprint:** Despite handling unbounded context, the model maintains a controlled memory footprint. This is achieved through efficient memory management techniques, making it practical for real-world applications where memory limitations are a concern.\n",
       "* **Infini-Attention Mechanism:**  The core of this model likely involves a novel attention mechanism called \"Infini-attention.\" This mechanism appears to combine both local and global context states, similar to multi-head attention but with the ability to handle extended sequences.\n",
       "\n",
       "**Possible Applications:**\n",
       "\n",
       "* **Long Document Summarization:** Analyzing and summarizing lengthy documents like research papers or books.\n",
       "* **Time-Series Analysis:** Processing and forecasting extensive time-series data, such as financial markets or climate patterns.\n",
       "* **Natural Language Understanding:**  Tasks involving understanding complex language structures and long-range dependencies within text. \n",
       "* **Code Generation and Analysis:**  Analyzing and generating code where understanding dependencies across long code sequences is crucial.\n",
       "\n",
       "**Additional Notes:**\n",
       "\n",
       "* The provided context mentions \"segment-level memory models\" and comparisons, suggesting that this model might be an improvement over existing approaches for handling long sequences.\n",
       "* The reference to Figure 1 and Table 1 implies that the full document likely contains visual illustrations and detailed comparisons with other models, which would provide a more comprehensive understanding. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\" What are Efficient Infinite Context Transformers\")\n",
    "\n",
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d21a4cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
